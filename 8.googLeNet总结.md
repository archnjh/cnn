• GoogLeNet的总体设计目标是什么？（有三个方面）
设计一个神经网络，增加深度和宽度（更深22层），但不增加 计算量（计算量是AlexNet的1/12），精度更好（2014年ImageNet比赛冠军）

• 直接去掉网络中较小权重的稀疏方法有什么缺陷？ 
从硬件角度分析，稀疏的硬件实现是低效的（大量的查找和缓存缺失）

• 从设计思路讲，Inception是如何实现稀疏的？
卷积核、batch个数大时，卷积也可稠密运算
稀疏矩阵聚类为稠密的子矩阵
用密集的卷积计算近似稀疏结构

• 如何将赫布理论应用到Inception的设计中？
赫布理论：如果两个神经元同步激发，则它们之间的权重增加；如果单独激发，则权重减少。
根据赫布理论，通过分析某些神经元之间激活值的相关性，将相关度高的神经元聚合，获取一个稀疏表示（利用1* 1卷积实现）

• Inception的结构大概是怎么样的？有几个分支，分别是什么？
 ①用1x1卷积实现赫布理论，把神经元聚在一起 
 ②为了增加聚类时的空间信息，使用patch更大的卷积核，个数相应减少 
 ③为了方便，使用1x1，3x3，5x5卷积，输出 combine 
 ④由于pooling效果很好，也添加并行的pooling分支
改进：用1x1卷积降维,相当于做embedding 
An embedding is **a relatively low-dimensional space into which you can translate high-dimensional vectors**.
做法： ①1x1卷积放在3x3和5x5之前降维 ②1x1卷积放在pooling后面降维 ③1x1后面有ReLU，增加非线性

• Inception是如何实现多尺度特征提取的？
Inception采用多尺度并列的结构，捕捉多尺度信息并融合，输出到下一层 这是因为，视觉信息应该在不同的尺度上进行处理，然后进行聚合，以便下一阶段能够同时从不 同的尺度上提取特征。

• Inception中1x1卷积的作用是什么？
降维，减少 计算量 在此基础上，就可以实现在不 增加计算量的情况下，增加卷 积神经网络的深度和宽度

• GoogLeNet中添加辅助loss的目的是什么？测试的时候是否还需要辅助loss？
在中间添加两个额外的softmax（辅助 loss），用于缓解梯度消失，这两个loss 的比重较小(0.3)，在测试时，这两个loss 会被去掉