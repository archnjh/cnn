• AlexNet中使用了什么激活函数？ReLU相比Sigmoid的主要优势有哪些？ 

ReLU不会发生饱和，缓解梯度消失，缓解过拟合

• Dropout 为什么可以减少过拟合？Dropout是怎么运算的？（提示，训练和测试时不同）

训练时：按0.5的概率随机把神经元置0（即 不参与前向传播和反向传播） 测试时：所有神经元都参与运算，输出乘0.5。相当于训练时使用不同的神经网络结构，这些网络共享 权重，这样可以减少神经元之间的依赖性， 从而学习到的特征效果好

![[Pasted image 20230204175209.png]]

• AlexNet的结构大概是什么样的，几个卷积层，几个全连接层？ 5 3

• AlexNet使用了哪些减少过拟合的方法？

ReLU、Dropout、重叠的MaxPooling、数据扩增（平移、翻转、改变RGB）

• AlexNet中的数据扩增是怎么做的？

第一种数据扩增：平移和水平翻转 训练时：从256x256图片中随机取 224x224和其水平翻转 样本数量可以扩增2048倍 测试时：取四个角和中心的224x224，和 其水平翻转，共10个图片的结果取平均
第二种数据扩增：改变RGB值 做法：利用PCA获得主成分（计算协方差矩阵的特征值 和特征向量），然后在特征值上乘高斯随机数 原始图像 [p1，p2，p3][λ1， λ2， λ3]T 变换后 [p1，p2，p3][α1λ1， α2λ2， α3λ3]T 这样做的合理性：利用了自然图像的形状，模拟物体在 光照变化下的不同图像

• AlexNet的网络结构中，有什么不合理的地方？（提示，如第一个卷积层）

第一层卷积核过大，增加了计算量

• AlexNet有什么后面人们几乎不再用的设计？（提示，如第一卷积层和LRN）

LRN 局部归一化 跟在前两层的ReLU之后